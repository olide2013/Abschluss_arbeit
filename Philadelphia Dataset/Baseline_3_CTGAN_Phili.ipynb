{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf8QMeOORHkX"
   },
   "source": [
    "## Importieren Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeweDlS8_g99"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code zum Hochladen von Dateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "YR_EKxKA_4fu",
    "outputId": "c386ce64-dce2-4b5b-e3c3-09ebbb36f181"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-a0083f1c-f0d6-4cce-97f9-1b78744f2979\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-a0083f1c-f0d6-4cce-97f9-1b78744f2979\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving new_philldata1.csv to new_philldata1.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verarbeitung großer Datenmengen mit Dask und Standardisierung numerischer Spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLHLCLzY_4mF",
    "outputId": "0fb674d6-076a-4f54-c86a-1fdef59ab3ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fy        fm  check_date   document_no      dept  \\\n",
      "0 NaN  0.941375  2017-04-12  CHEK17119771  1.319343   \n",
      "1 NaN  1.516865  2017-06-09  ACHD17177233  0.145450   \n",
      "2 NaN -0.497348  2016-11-25  CHEK17063736  1.466080   \n",
      "3 NaN -1.648327  2016-07-07  CHEK17000247 -0.955075   \n",
      "4 NaN -1.648327  2016-07-08  ACHD17000233 -0.074655   \n",
      "\n",
      "            department_title     char_            character_title sub_obj  \\\n",
      "0                42 COMMERCE -0.672462    02 PURCHASE OF SERVICES     231   \n",
      "1  26 LICENSES & INSPECTIONS -0.672462    02 PURCHASE OF SERVICES     211   \n",
      "2                     44 LAW -0.672462    02 PURCHASE OF SERVICES     258   \n",
      "3                  11 POLICE -0.672462    02 PURCHASE OF SERVICES     260   \n",
      "4                 23 PRISONS  0.605375  03 MATERIALS AND SUPPLIES     313   \n",
      "\n",
      "                         sub_obj_title                   vendor_name  \\\n",
      "0                  OVERTIME MEALS 0231                  EAT AT JOE'S   \n",
      "1                  TRANSPORTATION 0211   L & I Â TRAVEL IMPREST FUND   \n",
      "2                 COURT REPORTERS 0258  MARLENE BELL REPORTING, INC.   \n",
      "3  REPAIR AND MAINTENANCE CHARGES 0260    RICOH AMERICAS CORPORATION   \n",
      "4                            FOOD 0313          PHILADELPHIA PRISONS   \n",
      "\n",
      "  doc_ref_no_prefix doc_ref_no_prefix_definition contract_number  \\\n",
      "0              PVXX              payment voucher          150166   \n",
      "1              PVXX              payment voucher          160025   \n",
      "2              PVXX              payment voucher          140005   \n",
      "3              VCXX                  procurement          160047   \n",
      "4              PCXX                   petty cash          140005   \n",
      "\n",
      "                                contract_description  transaction_amount  \n",
      "0                          Water Treatment Chemicals           -0.040081  \n",
      "1   STAPLES CONTRACT & COMMERCIAL INC Bid # 010615NJ           -0.039942  \n",
      "2                          XEROX CORP Bid # PEPPM 15           -0.039193  \n",
      "3                          Water Treatment Chemicals           -0.040119  \n",
      "4  Purchase, Lease/Purchase, Lease and Maintenanc...           -0.040069  \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the dataset with explicit dtypes for problematic columns\n",
    "data_path = 'new_philldata1.csv'\n",
    "data = dd.read_csv(\n",
    "    data_path,\n",
    "    dtype={\n",
    "        'contract_number': 'object',\n",
    "        'sub_obj': 'object'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Select only numeric columns for standardization\n",
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "data_numeric = data[numeric_cols]\n",
    "\n",
    "# Standardize the numeric data\n",
    "data_numeric = data_numeric.map_partitions(\n",
    "    lambda df: (df - df.mean()) / df.std(),\n",
    "    meta={col: 'float64' for col in numeric_cols}\n",
    ")\n",
    "\n",
    "# Reassign standardized numeric columns to the original dataframe\n",
    "for col in numeric_cols:\n",
    "    data = data.assign(**{col: data_numeric[col]})\n",
    "\n",
    "# Convert only numeric columns to float32\n",
    "for col in numeric_cols:\n",
    "    data = data.assign(**{col: data[col].astype('float32')})\n",
    "\n",
    "# Compute the final result\n",
    "data = data.compute()\n",
    "\n",
    "# Check the result\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition von Generator und Kritiker (Critic) für ein GAN-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Be3G6mR_huU"
   },
   "outputs": [],
   "source": [
    "# Define the Generator and Critic\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, condition):\n",
    "        input_data = torch.cat([z, condition], dim=1)\n",
    "        return self.model(input_data)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        input_data = torch.cat([x, condition], dim=1)\n",
    "        return self.model(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berechnung der Gradientenstrafe (Gradient Penalty) für stabileres GAN-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maTOgv_K_h3Y"
   },
   "outputs": [],
   "source": [
    "# Gradient Penalty\n",
    "def gradient_penalty(critic, real_data, fake_data, condition):\n",
    "    alpha = torch.rand(real_data.size(0), 1).to(real_data.device)\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "    critic_output = critic(interpolates, condition)\n",
    "    gradients = grad(outputs=critic_output, inputs=interpolates,\n",
    "                     grad_outputs=torch.ones_like(critic_output),\n",
    "                     create_graph=True, retain_graph=True)[0]\n",
    "    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsschleife für CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRCMSnXp_h_k"
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_ctgan_wgan(generator, critic, data_loader, num_epochs, device):\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "    optimizer_C = optim.Adam(critic.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "    lambda_gp = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for real_data, condition in data_loader:\n",
    "            real_data, condition = real_data.to(device), condition.to(device)\n",
    "            batch_size = real_data.size(0)\n",
    "\n",
    "            # Train Critic\n",
    "            for _ in range(5):\n",
    "                z = torch.randn(batch_size, noise_dim).to(device)\n",
    "                fake_data = generator(z, condition).detach()\n",
    "                critic_real = critic(real_data, condition).mean()\n",
    "                critic_fake = critic(fake_data, condition).mean()\n",
    "                gp = gradient_penalty(critic, real_data, fake_data, condition)\n",
    "                loss_C = critic_fake - critic_real + lambda_gp * gp\n",
    "                optimizer_C.zero_grad()\n",
    "                loss_C.backward()\n",
    "                optimizer_C.step()\n",
    "\n",
    "            # Train Generator\n",
    "            z = torch.randn(batch_size, noise_dim).to(device)\n",
    "            fake_data = generator(z, condition)\n",
    "            loss_G = -critic(fake_data, condition).mean()\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkd9ECy7H1gw"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptAZcjQZD-Tg",
    "outputId": "549ffe01-46a4-45cf-9e14-99b3eeaa2a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     71695\n",
      "         1.0       1.00      1.00      1.00     71642\n",
      "\n",
      "    accuracy                           1.00    143337\n",
      "   macro avg       1.00      1.00      1.00    143337\n",
      "weighted avg       1.00      1.00      1.00    143337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Assuming 'data' holds your preprocessed real data\n",
    "# Select only numeric features for training\n",
    "numeric_features = data.select_dtypes(include=['number']).columns\n",
    "real_data = data[numeric_features].values  # Convert the 'data' DataFrame to a NumPy array, selecting only numeric columns\n",
    "\n",
    "# Generate your synthetic data (replace with your actual generator and parameters)\n",
    "# For example:\n",
    "# num_synthetic_samples = 10000  # Define the number of synthetic samples you want\n",
    "# noise = torch.randn(num_synthetic_samples, noise_dim).to(device)\n",
    "# conditions = torch.tensor(conditions_data.values.astype(np.float32)).to(device) # Replace conditions_data with your actual condition data\n",
    "# synthetic_data = generator(noise, conditions).detach().cpu().numpy()\n",
    "\n",
    "# Assuming 'synthetic_data' is now generated\n",
    "#  Generate synthetic data using your trained generator\n",
    "# For demonstration purposes, let's create a random synthetic dataset\n",
    "num_synthetic_samples = real_data.shape[0]  # Match the number of real samples\n",
    "num_features = real_data.shape[1]\n",
    "synthetic_data = np.random.rand(num_synthetic_samples, num_features) # Replace this with your actual synthetic data generation\n",
    "\n",
    "# Combine real and synthetic data\n",
    "real_labels = np.zeros(real_data.shape[0])  # Label for real data\n",
    "synthetic_labels = np.ones(synthetic_data.shape[0])  # Label for synthetic data\n",
    "\n",
    "# Combine the data and labels\n",
    "combined_data = np.vstack((real_data, synthetic_data))\n",
    "combined_labels = np.hstack((real_labels, synthetic_labels))\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data, combined_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj3GLDW2IiaB"
   },
   "source": [
    "## Decission tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25RjOihPD-bY",
    "outputId": "82c6a5e6-e092-4bbc-b413-ef7da734bd80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Real       1.00      1.00      1.00     71695\n",
      "   Synthetic       1.00      1.00      1.00     71642\n",
      "\n",
      "    accuracy                           1.00    143337\n",
      "   macro avg       1.00      1.00      1.00    143337\n",
      "weighted avg       1.00      1.00      1.00    143337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Combine real and synthetic data\n",
    "real_labels = np.zeros(real_data.shape[0])  # Label for real data\n",
    "synthetic_labels = np.ones(synthetic_data.shape[0])  # Label for synthetic data\n",
    "\n",
    "# Combine the data and labels\n",
    "data = np.vstack((real_data, synthetic_data))\n",
    "labels = np.hstack((real_labels, synthetic_labels))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Detailed Classification Report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"Real\", \"Synthetic\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f37-P7ZOI2gZ"
   },
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mmp8kvrvD-iw",
    "outputId": "97f6013b-5ce2-464a-b5d5-833cc4215dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Precision: 0.63\n",
      "Recall: 0.28\n",
      "F1 Score: 0.39\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.54      0.84      0.65     71695\n",
      "     Anomaly       0.63      0.28      0.39     71642\n",
      "\n",
      "    accuracy                           0.56    143337\n",
      "   macro avg       0.58      0.56      0.52    143337\n",
      "weighted avg       0.58      0.56      0.52    143337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Combine real and synthetic data\n",
    "real_labels = np.zeros(real_data.shape[0])  # Label for real data (normal)\n",
    "synthetic_labels = np.ones(synthetic_data.shape[0])  # Label for synthetic data (anomalies)\n",
    "\n",
    "# Combine data and labels\n",
    "data = np.vstack((real_data, synthetic_data))\n",
    "labels = np.hstack((real_labels, synthetic_labels))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=\"auto\", random_state=42)\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "# Isolation Forest predicts -1 for anomalies and 1 for normal points\n",
    "y_pred_raw = iso_forest.predict(X_test)\n",
    "y_pred = np.where(y_pred_raw == 1, 0, 1)  # Convert to 0 (normal) and 1 (anomaly)\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Detailed Classification Report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"Normal\", \"Anomaly\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fWnBa6aJRqc"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-QdlGWXD-q_",
    "outputId": "14e16456-1e78-429f-c81f-1196f2a28ef0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [09:05:13] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Real       1.00      1.00      1.00     71695\n",
      "   Synthetic       1.00      1.00      1.00     71642\n",
      "\n",
      "    accuracy                           1.00    143337\n",
      "   macro avg       1.00      1.00      1.00    143337\n",
      "weighted avg       1.00      1.00      1.00    143337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Combine real and synthetic data\n",
    "real_labels = np.zeros(real_data.shape[0])  # Label for real data\n",
    "synthetic_labels = np.ones(synthetic_data.shape[0])  # Label for synthetic data\n",
    "\n",
    "# Combine the data and labels\n",
    "data = np.vstack((real_data, synthetic_data))\n",
    "labels = np.hstack((real_labels, synthetic_labels))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train XGBoost Classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    use_label_encoder=False,  # Avoid warning for label encoding in new versions of XGBoost\n",
    "    eval_metric='logloss',   # Specify evaluation metric\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Detailed Classification Report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"Real\", \"Synthetic\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ujmXG1hJoR8"
   },
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXBCVs6OD-ym",
    "outputId": "74dd62bb-09ff-4cb9-c4e4-bc7bc9d72619",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 167252, number of negative: 167199\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027823 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 334451, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500079 -> initscore=0.000317\n",
      "[LightGBM] [Info] Start training from score 0.000317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\ttraining's binary_logloss: 2.1902e-05\tvalid_1's binary_logloss: 2.1902e-05\n",
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Real       1.00      1.00      1.00     71695\n",
      "   Synthetic       1.00      1.00      1.00     71642\n",
      "\n",
      "    accuracy                           1.00    143337\n",
      "   macro avg       1.00      1.00      1.00    143337\n",
      "weighted avg       1.00      1.00      1.00    143337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# Combine real and synthetic data\n",
    "real_labels = np.zeros(real_data.shape[0])  # Label for real data\n",
    "synthetic_labels = np.ones(synthetic_data.shape[0])  # Label for synthetic data\n",
    "\n",
    "# Combine the data and labels\n",
    "data = np.vstack((real_data, synthetic_data))\n",
    "labels = np.hstack((real_labels, synthetic_labels))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert data into LightGBM dataset format\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Define LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',        # Binary classification\n",
    "    'metric': 'binary_logloss',   # Evaluation metric\n",
    "    'boosting_type': 'gbdt',      # Gradient Boosting Decision Tree\n",
    "    'num_leaves': 31,             # Maximum number of leaves in one tree\n",
    "    'learning_rate': 0.1,         # Step size shrinkage\n",
    "    'feature_fraction': 0.9       # Randomly select a subset of features\n",
    "}\n",
    "\n",
    "# Use a callback for early stopping\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=10)]\n",
    "\n",
    "# Train the LightGBM model\n",
    "lgb_model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[train_data, test_data],  # Provide both training and testing data for validation\n",
    "    callbacks=callbacks                  # Use the early stopping callback\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_proba = lgb_model.predict(X_test)  # Probabilities\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Compute Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print Metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Detailed Classification Report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"Real\", \"Synthetic\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuhtjXhmR3fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmuYpbp2R3ll"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2G2rvSpR3y_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
